# -*- coding: utf-8 -*-
"""
CRNN è½¦ç‰Œè¯†åˆ«è®­ç»ƒè„šæœ¬ - CBLPRD-330k ä¸“ç”¨ (GPU ä¼˜åŒ– + æ–­ç‚¹ç»­è®­ + æ”¶æ•›åŠ é€Ÿ)
ã€å…³é”®ä¼˜åŒ–ã€‘
1. ä¼˜åŒ–å­¦ä¹ ç‡ç­–ç•¥ï¼šReduceLROnPlateau + å­¦ä¹ ç‡é¢„çƒ­
2. å¢åŠ è®­ç»ƒè½®æ•°ï¼šEPOCHS=30ï¼ˆ35ä¸‡å›¾ç‰‡æ¨èï¼‰
3. æ•°æ®å¢å¼ºï¼šéšæœºæ°´å¹³ç¿»è½¬ï¼ˆè½¦ç‰Œæ–¹å‘æ— å…³ï¼‰
4. æŸå¤±ç›‘æ§ï¼šéªŒè¯é›†æŸå¤±ä½œä¸ºå­¦ä¹ ç‡è°ƒæ•´ä¾æ®
5. ä¼˜åŒ–è®­ç»ƒå¾ªç¯ï¼šæ›´æœ‰æ•ˆçš„è¿›åº¦æ˜¾ç¤º
"""

import os
import cv2
import numpy as np
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import Dataset, DataLoader
import matplotlib.pyplot as plt
import time
import random

# ----------------------------
# è·¯å¾„é…ç½®ï¼šå®šä¹‰é¡¹ç›®æ ¹ç›®å½•ã€å›¾åƒæ–‡ä»¶å¤¹ã€è®­ç»ƒ/éªŒè¯åˆ—è¡¨åŠæ£€æŸ¥ç‚¹è·¯å¾„
# ----------------------------
ROOT_DIR = os.getcwd()  # è·å–å½“å‰å·¥ä½œç›®å½•
IMG_DIR = os.path.join(ROOT_DIR, "CBLPRD-330k")  # å›¾åƒæ•°æ®é›†è·¯å¾„
TRAIN_TXT = os.path.join(ROOT_DIR, "train.txt")   # è®­ç»ƒé›†æ ‡ç­¾æ–‡ä»¶
VAL_TXT = os.path.join(ROOT_DIR, "val.txt")       # éªŒè¯é›†æ ‡ç­¾æ–‡ä»¶
CHECKPOINT_PATH = os.path.join(ROOT_DIR, "checkpoint.pth")  # æ–­ç‚¹ç»­è®­ä¿å­˜è·¯å¾„

print(f"ğŸ“ é¡¹ç›®ç›®å½•: {ROOT_DIR}")
# æ–­è¨€æ£€æŸ¥ï¼šç¡®ä¿å…³é”®è·¯å¾„å­˜åœ¨ï¼Œå¦åˆ™æŠ¥é”™é€€å‡º
assert os.path.exists(IMG_DIR), f"âŒ å›¾åƒæ–‡ä»¶å¤¹ä¸å­˜åœ¨: {IMG_DIR}"
assert os.path.exists(TRAIN_TXT), f"âŒ train.txt ä¸å­˜åœ¨: {TRAIN_TXT}"
assert os.path.exists(VAL_TXT), f"âŒ val.txt ä¸å­˜åœ¨: {VAL_TXT}"

# ----------------------------
# å­—ç¬¦é›†å®šä¹‰ï¼šæ”¯æŒä¸­å›½è½¦ç‰Œæ‰€æœ‰åˆæ³•å­—ç¬¦ï¼ˆçœä»½+å­—æ¯+æ•°å­—+ç‰¹æ®Šæ ‡è¯†ï¼‰
# ----------------------------
PROVINCES = [
    'äº¬', 'æ²ª', 'æ´¥', 'æ¸', 'å†€', 'æ™‹', 'è’™', 'è¾½', 'å‰', 'é»‘',
    'è‹', 'æµ™', 'çš–', 'é—½', 'èµ£', 'é²', 'è±«', 'é„‚', 'æ¹˜', 'ç²¤',
    'æ¡‚', 'ç¼', 'å·', 'è´µ', 'äº‘', 'è—', 'é™•', 'ç”˜', 'é’', 'å®', 'æ–°',
    'æ¸¯', 'æ¾³', 'æŒ‚', 'å­¦', 'é¢†', 'ä½¿', 'ä¸´'
]
LETTERS = [chr(ord('A') + i) for i in range(26)]  # A-Z
DIGITS = [str(i) for i in range(10)]              # 0-9

# æ„å»ºå®Œæ•´å­—ç¬¦è¡¨ï¼šç´¢å¼•0ä¸ºCTCçš„blankæ ‡è®°
CHARS = ['<blank>'] + PROVINCES + LETTERS + DIGITS
CHAR2IDX = {ch: idx for idx, ch in enumerate(CHARS)}  # å­—ç¬¦ â†’ ç´¢å¼•
IDX2CHAR = {idx: ch for ch, idx in CHAR2IDX.items()}  # ç´¢å¼• â†’ å­—ç¬¦
NUM_CLASSES = len(CHARS)

print(f"ğŸ”¤ å­—ç¬¦é›†å¤§å°: {NUM_CLASSES} (å« blank)")

# ----------------------------
# è¶…å‚æ•°é…ç½®ï¼šæ ¹æ®GPUæ˜¾å­˜å’Œæ•°æ®è§„æ¨¡ä¼˜åŒ–
# ----------------------------
BATCH_SIZE = 32        # æ‰¹æ¬¡å¤§å°ï¼ˆæ˜¾å­˜å…è®¸ä¸‹å°½é‡å¤§ï¼‰
EPOCHS = 30            # æ€»è®­ç»ƒè½®æ•°ï¼ˆå¤§æ•°æ®é›†éœ€æ›´å¤šepochï¼‰
LEARNING_RATE = 0.0005 # åˆå§‹å­¦ä¹ ç‡ï¼ˆè¾ƒå°å€¼åˆ©äºæ”¶æ•›ï¼‰
IMG_HEIGHT = 32        # è¾“å…¥å›¾åƒé«˜åº¦ï¼ˆCRNNè¦æ±‚é«˜åº¦å›ºå®šï¼‰
IMG_WIDTH = 280        # è¾“å…¥å›¾åƒå®½åº¦ï¼ˆè¶³å¤Ÿå®¹çº³7ä½è½¦ç‰Œï¼‰
LOG_INTERVAL = 1000    # æ¯éš”å¤šå°‘batchæ‰“å°æ—¥å¿—
DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
print(f"ğŸ–¥ï¸ ä½¿ç”¨è®¾å¤‡: {DEVICE} | BATCH_SIZE={BATCH_SIZE} | EPOCHS={EPOCHS}")

# ----------------------------
# CRNN æ¨¡å‹å®šä¹‰ï¼šCNN + BiLSTM + FCï¼Œä¸“ä¸ºä¸å®šé•¿æ–‡æœ¬è¯†åˆ«è®¾è®¡
# ----------------------------
class CRNN(nn.Module):
    def __init__(self, num_classes, imgH=32, nc=1, nh=256):
        super(CRNN, self).__init__()
        assert imgH % 16 == 0, 'imgHå¿…é¡»æ˜¯16çš„å€æ•°ï¼ˆå› CNNä¸‹é‡‡æ ·ï¼‰'
        
        # CNN ç‰¹å¾æå–å™¨ï¼šé€æ­¥ä¸‹é‡‡æ ·ï¼Œæœ€ç»ˆé«˜åº¦å‹ç¼©ä¸º1
        self.cnn = nn.Sequential(
            nn.Conv2d(nc, 64, 3, 1, 1), nn.ReLU(True), nn.MaxPool2d(2, 2),
            nn.Conv2d(64, 128, 3, 1, 1), nn.ReLU(True), nn.MaxPool2d(2, 2),
            nn.Conv2d(128, 256, 3, 1, 1), nn.BatchNorm2d(256), nn.ReLU(True),
            nn.Conv2d(256, 256, 3, 1, 1), nn.ReLU(True), nn.MaxPool2d((2, 2), (2, 1), (0, 1)),
            nn.Conv2d(256, 512, 3, 1, 1), nn.BatchNorm2d(512), nn.ReLU(True),
            nn.Conv2d(512, 512, 3, 1, 1), nn.ReLU(True), nn.MaxPool2d((2, 2), (2, 1), (0, 1)),
            nn.Conv2d(512, 512, 2, 1, 0), nn.ReLU(True)  # æœ€ç»ˆè¾“å‡º [B, 512, 1, W']
        )
        # åŒå‘LSTMï¼šæ•æ‰å­—ç¬¦ä¸Šä¸‹æ–‡ä¾èµ–
        self.rnn = nn.LSTM(512, nh, num_layers=2, bidirectional=True, batch_first=True)
        # å…¨è¿æ¥å±‚ï¼šæ˜ å°„åˆ°å­—ç¬¦ç±»åˆ«ç©ºé—´
        self.fc = nn.Linear(nh * 2, num_classes)
        
        # æƒé‡åˆå§‹åŒ–ï¼šä½¿ç”¨He/Xavieræå‡æ”¶æ•›é€Ÿåº¦
        self._initialize_weights()

    def _initialize_weights(self):
        """å¯¹å·ç§¯å±‚ã€çº¿æ€§å±‚ã€LSTMè¿›è¡Œåˆç†åˆå§‹åŒ–"""
        for m in self.modules():
            if isinstance(m, nn.Conv2d) or isinstance(m, nn.Linear):
                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')
                if m.bias is not None:
                    nn.init.constant_(m.bias, 0)
        for name, param in self.rnn.named_parameters():
            if 'bias' in name:
                nn.init.constant_(param, 0.0)
            elif 'weight' in name:
                nn.init.xavier_uniform_(param)

    def forward(self, x):
        """å‰å‘ä¼ æ’­ï¼šè¾“å…¥ [B, 1, H, W] â†’ è¾“å‡º [B, T, num_classes]"""
        conv = self.cnn(x)  # [B, 512, 1, W']
        b, c, h, w = conv.size()
        assert h == 1, "CNNè¾“å‡ºé«˜åº¦å¿…é¡»ä¸º1"
        rnn_input = conv.squeeze(2).permute(0, 2, 1)  # [B, W', 512]
        rnn_out, _ = self.rnn(rnn_input)  # [B, W', 512]
        output = self.fc(rnn_out)  # [B, W', num_classes]
        return output

# ----------------------------
# è‡ªå®šä¹‰æ•°æ®é›†ç±»ï¼šæ”¯æŒè®­ç»ƒ/éªŒè¯æ¨¡å¼ã€æ•°æ®æ¸…æ´—ã€å¢å¼º
# ----------------------------
class LicensePlateDataset(Dataset):
    def __init__(self, txt_path, img_dir, debug=False, is_train=True):
        self.img_dir = img_dir
        self.debug = debug
        self.is_train = is_train  # æ ‡è®°æ˜¯å¦ä¸ºè®­ç»ƒé›†ï¼ˆå†³å®šæ˜¯å¦å¢å¼ºï¼‰
        self.data = []
        # è¯»å–æ ‡ç­¾æ–‡ä»¶ï¼šæ¯è¡Œæ ¼å¼ "filename.jpg label"
        with open(txt_path, 'r', encoding='utf-8') as f:
            for line in f:
                parts = line.strip().split()
                if len(parts) >= 2:
                    img_path_in_txt = parts[0]
                    label = parts[1]
                    filename = os.path.basename(img_path_in_txt)
                    full_path = os.path.join(img_dir, filename)
                    if os.path.exists(full_path):
                        self.data.append((filename, label))
                    elif debug:
                        print(f"âš ï¸ å›¾åƒä¸å­˜åœ¨: {full_path}")
        print(f"âœ… åŠ è½½ {len(self.data)} æ¡æœ‰æ•ˆæ ·æœ¬ from {txt_path} (è®­ç»ƒé›†={self.is_train})")

    def __len__(self):
        return len(self.data)

    def __getitem__(self, idx):
        filename, label = self.data[idx]
        img_path = os.path.join(self.img_dir, filename)
        image = cv2.imread(img_path, cv2.IMREAD_GRAYSCALE)  # ç°åº¦å›¾èŠ‚çœæ˜¾å­˜
        if image is None:
            image = np.zeros((IMG_HEIGHT, IMG_WIDTH), dtype=np.uint8)
        else:
            image = cv2.resize(image, (IMG_WIDTH, IMG_HEIGHT))  # ç»Ÿä¸€å°ºå¯¸
        
        # æ¸…æ´—æ ‡ç­¾ï¼šåªä¿ç•™åˆæ³•å­—ç¬¦ï¼Œé¿å…æ— æ•ˆæ ‡ç­¾å¯¼è‡´å´©æºƒ
        valid_chars = set(CHARS[1:])  # æ’é™¤ <blank>
        cleaned_label = ''.join(ch for ch in label if ch in valid_chars)
        if len(cleaned_label) == 0:
            cleaned_label = "äº¬A00000"  # é»˜è®¤å…œåº•
        
        # å°†å­—ç¬¦è½¬ä¸ºç´¢å¼•åºåˆ—
        label_indices = [CHAR2IDX[ch] for ch in cleaned_label]
        
        # è½¬ä¸ºPyTorchå¼ é‡å¹¶å½’ä¸€åŒ–åˆ° [-1, 1]
        image = torch.from_numpy(image).float().unsqueeze(0)  # [1, H, W]
        image = image / 255.0
        image = (image - 0.5) / 0.5  # æ ‡å‡†åŒ–
        
        # âœ… æ•°æ®å¢å¼ºï¼šä»…è®­ç»ƒé›†å¯ç”¨éšæœºæ°´å¹³ç¿»è½¬ï¼ˆè½¦ç‰Œå¯¹ç§°ï¼‰
        if self.is_train and random.random() > 0.5:
            image = torch.flip(image, [2])  # æ²¿å®½åº¦ç»´åº¦ç¿»è½¬
        
        return image, label_indices, cleaned_label

# ----------------------------
# æ‰¹æ¬¡åˆå¹¶å‡½æ•°ï¼šå°†å˜é•¿æ ‡ç­¾åºåˆ—å±•å¹³ï¼Œä¾›CTCLossä½¿ç”¨
# ----------------------------
def collate_fn(batch):
    """
    è¾“å…¥: [(img, label_indices, label_str), ...]
    è¾“å‡º: 
        images: [B, 1, H, W]
        targets: [sum(label_lengths)] æ‰€æœ‰æ ‡ç­¾æ‹¼æ¥æˆä¸€ç»´
        target_lengths: [B] æ¯ä¸ªæ ·æœ¬çš„æ ‡ç­¾é•¿åº¦
        labels: [B] åŸå§‹å­—ç¬¦ä¸²æ ‡ç­¾ï¼ˆç”¨äºè®¡ç®—å‡†ç¡®ç‡ï¼‰
    """
    images, label_indices_list, labels = zip(*batch)
    images = torch.stack(images, 0)  # åˆå¹¶å›¾åƒ
    
    flat_targets = []
    target_lengths = []
    for indices in label_indices_list:
        flat_targets.extend(indices)
        target_lengths.append(len(indices))
    
    targets = torch.LongTensor(flat_targets)
    target_lengths = torch.IntTensor(target_lengths)
    return images, targets, target_lengths, labels

# ----------------------------
# CTCè§£ç å‡½æ•°ï¼šå°†æ¨¡å‹è¾“å‡ºè½¬æ¢ä¸ºå¯è¯»å­—ç¬¦ä¸²ï¼ˆå»é™¤blankå’Œé‡å¤ï¼‰
# ----------------------------
def decode_ctc(outputs, output_lengths=None):
    """
    outputs: [B, T, num_classes] æ¨¡å‹åŸå§‹è¾“å‡º
    output_lengths: [B] æ¯ä¸ªæ ·æœ¬çš„æœ‰æ•ˆæ—¶é—´æ­¥ï¼ˆå¯é€‰ï¼‰
    è¿”å›: è§£ç åçš„å­—ç¬¦ä¸²åˆ—è¡¨
    """
    _, preds = outputs.max(2)  # [B, T]
    preds = preds.cpu().numpy()
    
    decoded_strings = []
    B = preds.shape[0]
    
    if output_lengths is None:
        seq_lengths = [preds.shape[1]] * B
    else:
        output_lengths = output_lengths.cpu().numpy()
        if len(output_lengths) != B:
            # å¤„ç†é•¿åº¦ä¸åŒ¹é…ï¼ˆå¦‚æœ€åä¸€ä¸ªbatchä¸è¶³ï¼‰
            seq_lengths = output_lengths[:B] if len(output_lengths) > B else np.concatenate([output_lengths, [preds.shape[1]] * (B - len(output_lengths))])
        else:
            seq_lengths = output_lengths

    for i in range(B):
        length = int(seq_lengths[i])
        seq = preds[i][:length]
        out = []
        prev = -1
        for p in seq:
            if p != prev and p != 0:  # è·³è¿‡blank(0)å’Œè¿ç»­é‡å¤
                out.append(IDX2CHAR[p])
            prev = p
        decoded_strings.append(''.join(out))
    return decoded_strings

# ----------------------------
# æ£€æŸ¥ç‚¹ä¿å­˜ä¸åŠ è½½ï¼šæ”¯æŒæ–­ç‚¹ç»­è®­
# ----------------------------
def save_checkpoint(epoch, model, optimizer, scheduler, best_acc, train_losses, val_losses, val_accuracies, path):
    """ä¿å­˜è®­ç»ƒçŠ¶æ€ï¼Œä¾¿äºæ¢å¤"""
    checkpoint = {
        'epoch': epoch,
        'model_state_dict': model.state_dict(),
        'optimizer_state_dict': optimizer.state_dict(),
        'scheduler_state_dict': scheduler.state_dict(),
        'best_acc': best_acc,
        'train_losses': train_losses,
        'val_losses': val_losses,
        'val_accuracies': val_accuracies
    }
    torch.save(checkpoint, path)
    print(f"ğŸ’¾ æ£€æŸ¥ç‚¹å·²ä¿å­˜è‡³: {path}")

def load_checkpoint(path, model, optimizer, scheduler):
    """ä»æ£€æŸ¥ç‚¹æ¢å¤è®­ç»ƒ"""
    if not os.path.exists(path):
        print("ğŸ” æœªæ‰¾åˆ°æ£€æŸ¥ç‚¹ï¼Œä»å¤´å¼€å§‹è®­ç»ƒ")
        return 0, 0.0, [], [], []
    
    checkpoint = torch.load(path, map_location=DEVICE)
    model.load_state_dict(checkpoint['model_state_dict'])
    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])
    scheduler.load_state_dict(checkpoint['scheduler_state_dict'])
    
    print(f"âœ… å·²ä» {path} æ¢å¤è®­ç»ƒï¼Œç»§ç»­ä» Epoch {checkpoint['epoch'] + 1} å¼€å§‹")
    return (
        checkpoint['epoch'] + 1,
        checkpoint['best_acc'],
        checkpoint['train_losses'],
        checkpoint['val_losses'],
        checkpoint['val_accuracies']
    )

# ----------------------------
# ä¸»è®­ç»ƒå‡½æ•°ï¼šåŒ…å«å­¦ä¹ ç‡è°ƒåº¦ã€éªŒè¯ã€ä¿å­˜æœ€ä½³æ¨¡å‹ç­‰
# ----------------------------
def train(resume=True):
    print("ğŸ” åŠ è½½è®­ç»ƒé›†...")
    train_dataset = LicensePlateDataset(TRAIN_TXT, IMG_DIR, debug=True, is_train=True)
    print("ğŸ” åŠ è½½éªŒè¯é›†...")
    val_dataset = LicensePlateDataset(VAL_TXT, IMG_DIR, debug=False, is_train=False)

    # åˆ›å»ºDataLoaderï¼šå¯ç”¨å¤šè¿›ç¨‹åŠ é€Ÿæ•°æ®åŠ è½½
    train_loader = DataLoader(
        train_dataset,
        batch_size=BATCH_SIZE,
        shuffle=True,
        num_workers=4,      # å¹¶è¡ŒåŠ è½½æ•°æ®
        pin_memory=True,    # GPUåŠ é€Ÿ
        collate_fn=collate_fn
    )
    val_loader = DataLoader(
        val_dataset,
        batch_size=BATCH_SIZE,
        shuffle=False,
        num_workers=4,
        pin_memory=True,
        collate_fn=collate_fn
    )

    # åˆå§‹åŒ–æ¨¡å‹ã€æŸå¤±å‡½æ•°ã€ä¼˜åŒ–å™¨
    model = CRNN(num_classes=NUM_CLASSES).to(DEVICE)
    criterion = nn.CTCLoss(blank=0, reduction='mean', zero_infinity=True)  # CTCæŸå¤±
    optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)
    
    # âœ… å­¦ä¹ ç‡è°ƒåº¦ï¼šå…ˆé¢„çƒ­5è½®ï¼Œå†æ ¹æ®éªŒè¯æŸå¤±åŠ¨æ€è°ƒæ•´
    warmup_epochs = 5
    scheduler_warmup = optim.lr_scheduler.LambdaLR(
        optimizer,
        lr_lambda=lambda epoch: min(1.0, (epoch + 1) / warmup_epochs)
    )
    scheduler_plateau = optim.lr_scheduler.ReduceLROnPlateau(
        optimizer, mode='min', factor=0.5, patience=3, verbose=True
    )

    # åˆå§‹åŒ–è®­ç»ƒçŠ¶æ€
    start_epoch = 0
    best_acc = 0.0
    train_losses = []
    val_losses = []
    val_accuracies = []

    # å°è¯•åŠ è½½æ£€æŸ¥ç‚¹
    if resume and os.path.exists(CHECKPOINT_PATH):
        start_epoch, best_acc, train_losses, val_losses, val_accuracies = load_checkpoint(
            CHECKPOINT_PATH, model, optimizer, scheduler_plateau
        )

    total_train_batches = len(train_loader)
    print(f"\nğŸš€ å¼€å§‹è®­ç»ƒ... æ€»å…± {total_train_batches} batches/epoch | EPOCHS={EPOCHS}\n")

    for epoch in range(start_epoch, EPOCHS):
        epoch_start = time.time()
        model.train()
        total_loss = 0.0
        batch_correct = 0
        batch_total = 0
        
        # è®­ç»ƒå¾ªç¯
        for i, (images, targets, target_lengths, labels) in enumerate(train_loader):
            images = images.to(DEVICE, non_blocking=True)
            targets = targets.to(DEVICE, non_blocking=True)
            target_lengths = target_lengths.to(DEVICE, non_blocking=True)

            # å‰å‘ä¼ æ’­
            outputs = model(images)  # [B, T, num_classes]
            output_lengths = torch.full((images.size(0),), outputs.size(1), dtype=torch.long)
            outputs_logprob = outputs.log_softmax(2).permute(1, 0, 2)  # [T, B, C] for CTC
            loss = criterion(outputs_logprob, targets, output_lengths, target_lengths)
            
            # åå‘ä¼ æ’­
            optimizer.zero_grad()
            loss.backward()
            optimizer.step()
            total_loss += loss.item()

            # å®æ—¶è®¡ç®—è®­ç»ƒå‡†ç¡®ç‡ï¼ˆç”¨äºæ—¥å¿—ï¼‰
            with torch.no_grad():
                preds = decode_ctc(outputs)
                for pred, gt in zip(preds, labels):
                    if pred == gt:
                        batch_correct += 1
                    batch_total += 1

            # æ—¥å¿—è¾“å‡º
            if (i + 1) % LOG_INTERVAL == 0 or i == len(train_loader) - 1:
                avg_loss = total_loss / (i + 1)
                batch_acc = batch_correct / batch_total if batch_total > 0 else 0.0
                print(f"Epoch {epoch+1}/{EPOCHS} | Batch {i+1}/{total_train_batches} | "
                      f"Loss: {loss.item():.4f} | Avg Loss: {avg_loss:.4f} | "
                      f"Batch Acc: {batch_acc:.4f} ({batch_correct}/{batch_total})")
        
        # è®¡ç®—å¹³å‡è®­ç»ƒæŸå¤±
        avg_train_loss = total_loss / total_train_batches
        train_losses.append(avg_train_loss)

        # éªŒè¯é˜¶æ®µ
        model.eval()
        val_loss = 0.0
        correct = 0
        total = 0
        with torch.no_grad():
            for images, targets, target_lengths, labels in val_loader:
                images = images.to(DEVICE)
                outputs = model(images)
                output_lengths = torch.full((images.size(0),), outputs.size(1), dtype=torch.long)
                outputs_logprob = outputs.log_softmax(2).permute(1, 0, 2)
                loss = criterion(outputs_logprob, targets, output_lengths, target_lengths)
                val_loss += loss.item()
                
                preds = decode_ctc(outputs)
                for pred, gt in zip(preds, labels):
                    if pred == gt:
                        correct += 1
                    total += 1

        val_loss = val_loss / len(val_loader)
        val_losses.append(val_loss)
        val_acc = correct / total
        val_accuracies.append(val_acc)
        
        # æ›´æ–°å­¦ä¹ ç‡ï¼šé¢„çƒ­æœŸååˆ‡æ¢ä¸ºPlateauè°ƒåº¦
        if epoch < warmup_epochs:
            scheduler_warmup.step()
        else:
            scheduler_plateau.step(val_loss)  # å…³é”®ï¼šç”¨éªŒè¯æŸå¤±è°ƒæ•´
        
        # æ‰“å°epochæ€»ç»“
        epoch_time = time.time() - epoch_start
        print(f"\nâœ… Epoch {epoch+1} å®Œæˆ | "
              f"Train Loss: {avg_train_loss:.4f} | "
              f"Val Loss: {val_loss:.4f} | "
              f"Val Acc: {val_acc:.4f} | "
              f"Time: {epoch_time:.2f}s")
        
        # ä¿å­˜æœ€ä½³æ¨¡å‹ï¼ˆæŒ‰éªŒè¯å‡†ç¡®ç‡ï¼‰
        if val_acc > best_acc:
            best_acc = val_acc
            torch.save(model.state_dict(), os.path.join(ROOT_DIR, "crnn_best.pth"))
            print(f"ğŸ‰ ä¿å­˜æœ€ä½³æ¨¡å‹! Acc: {best_acc:.4f} (Val Loss: {val_loss:.4f})\n")

        # ä¿å­˜å®Œæ•´æ£€æŸ¥ç‚¹ï¼ˆç”¨äºæ–­ç‚¹ç»­è®­ï¼‰
        save_checkpoint(
            epoch, model, optimizer, scheduler_plateau, best_acc, 
            train_losses, val_losses, val_accuracies, CHECKPOINT_PATH
        )

    # ç»˜åˆ¶è®­ç»ƒæ›²çº¿å¹¶ä¿å­˜
    plt.figure(figsize=(12, 8))
    plt.subplot(2, 1, 1)
    plt.plot(train_losses, label='Train Loss')
    plt.plot(val_losses, label='Val Loss', color='orange')
    plt.title('Training & Validation Loss'); plt.xlabel('Epoch'); plt.legend()

    plt.subplot(2, 1, 2)
    plt.plot(val_accuracies, label='Val Accuracy', color='green')
    plt.title('Validation Accuracy'); plt.xlabel('Epoch'); plt.legend()

    plt.tight_layout()
    plt.savefig(os.path.join(ROOT_DIR, "training_curves.png"))
    plt.show()

    print(f"\nğŸ¯ è®­ç»ƒå®Œæˆï¼æœ€ä½³å‡†ç¡®ç‡: {best_acc:.4f} | æœ€ä½³éªŒè¯æŸå¤±: {val_loss:.4f}")
    return model

# ----------------------------
# æµ‹è¯•å‡½æ•°ï¼šè¯„ä¼°æœ€ç»ˆæ¨¡å‹æ€§èƒ½
# ----------------------------
def test(model, val_loader, device):
    print("\nğŸ” å¼€å§‹æœ€ç»ˆæµ‹è¯•è¯„ä¼°...")
    model.eval()
    correct = 0
    total = 0
    examples = []

    with torch.no_grad():
        for images, targets, target_lengths, labels in val_loader:
            images = images.to(device)
            outputs = model(images)
            preds = decode_ctc(outputs)
            for pred, gt in zip(preds, labels):
                if pred == gt:
                    correct += 1
                total += 1
                if len(examples) < 5:
                    examples.append((gt, pred))

    test_acc = correct / total
    print(f"\nâœ… æœ€ç»ˆæµ‹è¯•å‡†ç¡®ç‡: {test_acc:.4f} ({correct}/{total})")

    print("\nğŸ“Š é¢„æµ‹æ ·ä¾‹ï¼ˆçœŸå® â†’ é¢„æµ‹ï¼‰:")
    for gt, pred in examples:
        status = "âœ…" if gt == pred else "âŒ"
        print(f"  {status} {gt} â†’ {pred}")

    return test_acc

# ----------------------------
# ä¸»ç¨‹åºå…¥å£
# ----------------------------
if __name__ == "__main__":
    trained_model = train(resume=True)

    print("\nğŸ“¥ åŠ è½½æœ€ä½³æ¨¡å‹è¿›è¡Œæœ€ç»ˆæµ‹è¯•...")
    best_model = CRNN(num_classes=NUM_CLASSES).to(DEVICE)
    best_model.load_state_dict(torch.load(os.path.join(ROOT_DIR, "crnn_best.pth"), map_location=DEVICE))
    
    val_dataset_final = LicensePlateDataset(VAL_TXT, IMG_DIR, debug=False, is_train=False)
    val_loader_final = DataLoader(
        val_dataset_final,
        batch_size=BATCH_SIZE,
        shuffle=False,
        num_workers=4,
        pin_memory=True,
        collate_fn=collate_fn
    )

    final_acc = test(best_model, val_loader_final, DEVICE)